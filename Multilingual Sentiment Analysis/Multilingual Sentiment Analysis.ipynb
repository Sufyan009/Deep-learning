{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9efa287b-cbd2-4b99-8c64-77a70021636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Al hamad\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, classification_report  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e852ae5-58b7-4898-8515-f1c59347e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "validation_df = pd.read_csv(\"validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61ab9018-f0fb-4e34-abae-6e0bfbc3dfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'review_id', 'product_id', 'reviewer_id', 'stars',\n",
       "       'review_body', 'review_title', 'language', 'product_category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "037973c3-e15e-4357-a7ec-a0c257b76ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de' 'en' 'es' 'fr' 'ja' 'zh']\n"
     ]
    }
   ],
   "source": [
    "unique_languages = train_df['language'].unique()\n",
    "print(unique_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5a253b8-d78f-4fcd-babd-824f44ae6909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "de    200000\n",
      "en    200000\n",
      "es    200000\n",
      "fr    200000\n",
      "ja    200000\n",
      "zh    200000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "language_counts = train_df['language'].value_counts()\n",
    "print(language_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70f16b0-9e26-45eb-b6c0-a5a3eb6ecf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stars\n",
      "1    240000\n",
      "2    240000\n",
      "3    240000\n",
      "4    240000\n",
      "5    240000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "stars_counts = train_df['stars'].value_counts()\n",
    "print(stars_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ded9cc4-7fc5-4506-bce3-265aa40bc577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Al hamad\\AppData\\Local\\Temp\\ipykernel_21884\\1608371494.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('language', group_keys=False).apply(\n",
      "C:\\Users\\Al hamad\\AppData\\Local\\Temp\\ipykernel_21884\\1608371494.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('language', group_keys=False).apply(\n",
      "C:\\Users\\Al hamad\\AppData\\Local\\Temp\\ipykernel_21884\\1608371494.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('language', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing Function\n",
    "def preprocess_data(df, sample_frac=0.20, max_samples=10000):\n",
    "    # Drop unnecessary columns\n",
    "    cols_to_drop = ['Unnamed: 0', 'review_id', 'product_id', 'reviewer_id', 'product_category']\n",
    "    df = df.drop(columns=[col for col in cols_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "    # Handle missing values\n",
    "    df['review_title'] = df['review_title'].fillna('')\n",
    "    df['review_body'] = df['review_body'].fillna('')\n",
    "    df['stars'] = df['stars'].fillna(3).astype(int)\n",
    "\n",
    "    # Combine and clean text\n",
    "    df['full_review'] = df['review_title'] + ' ' + df['review_body']\n",
    "    df['full_review'] = df['full_review'].apply(\n",
    "        lambda x: re.sub(r'[^\\p{L}\\s]', '', x).lower().strip()\n",
    "    )\n",
    "    df = df[df['full_review'].str.split().str.len() >= 3]\n",
    "    df = df.drop_duplicates(subset=['full_review'])\n",
    "\n",
    "    # Create sentiment labels\n",
    "    df[\"sentiment\"] = df[\"stars\"].apply(\n",
    "        lambda x: \"Positive\" if x >= 4 else \"Neutral\" if x == 3 else \"Negative\"\n",
    "    )\n",
    "    sentiment_mapping = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "    df[\"label\"] = df[\"sentiment\"].map(sentiment_mapping)\n",
    "\n",
    "    # Subsample while maintaining language distribution\n",
    "    if 'language' in df.columns:\n",
    "        df = df.groupby('language', group_keys=False).apply(\n",
    "            lambda x: x.sample(frac=sample_frac, random_state=42)\n",
    "        )\n",
    "\n",
    "    # Limit total samples to max_samples\n",
    "    df = df.sample(n=min(len(df), max_samples), random_state=42)\n",
    "\n",
    "    return df[['full_review', 'label', 'language', 'sentiment']]\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df = preprocess_data(train_df)\n",
    "test_df = preprocess_data(test_df)\n",
    "validation_df = preprocess_data(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d7d8534-1a67-4429-a5af-5f7c26a20643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10000 entries, 739386 to 829894\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   full_review  10000 non-null  object\n",
      " 1   label        10000 non-null  int64 \n",
      " 2   language     10000 non-null  object\n",
      " 3   sentiment    10000 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 390.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44d7f363-e3fb-4c6f-a596-5a8d792c92f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a smaller, faster model\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb73d129-c3c0-4434-9f27-2e494bb1fff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1327de01ecec42d3908c610f0402f565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0aff9bf6fd44f5b8de29f6948beb3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4765 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0f2eb8d1864596b753927ed6d3d1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4770 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert data to Hugging Face dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df[['full_review', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['full_review', 'label']])\n",
    "val_dataset = Dataset.from_pandas(validation_df[['full_review', 'label']])\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['full_review'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d505d82b-0a68-406b-aa47-2a560e46dd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Al hamad\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    "    id2label={0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"},\n",
    "    label2id={\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    ")\n",
    "\n",
    "\n",
    "# Training arguments optimized for speed\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),  # Enable FP16 for faster training if GPU is available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d105a99f-3827-4162-a970-e31261389a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 17:30:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.747800</td>\n",
       "      <td>0.652451</td>\n",
       "      <td>0.735010</td>\n",
       "      <td>0.608442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.601879</td>\n",
       "      <td>0.731656</td>\n",
       "      <td>0.686679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.470700</td>\n",
       "      <td>0.603996</td>\n",
       "      <td>0.762055</td>\n",
       "      <td>0.698636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1875, training_loss=0.6478162394205729, metrics={'train_runtime': 63061.9871, 'train_samples_per_second': 0.476, 'train_steps_per_second': 0.03, 'total_flos': 1973350632960000.0, 'train_loss': 0.6478162394205729, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(p.label_ids, preds),\n",
    "        \"macro_f1\": classification_report(\n",
    "            p.label_ids, preds,\n",
    "            target_names=[\"Negative\", \"Neutral\", \"Positive\"],\n",
    "            output_dict=True\n",
    "        )[\"macro avg\"][\"f1-score\"]\n",
    "    }\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f575294c-047b-47e3-8f56-e09eee6c1ebb",
   "metadata": {},
   "source": [
    "### **Analysis of Model Performance**  \n",
    "\n",
    "#### ✅ **Improvements**  \n",
    "1. **Training Loss Decreasing** – From **0.7478 → 0.5800 → 0.4707**  \n",
    "   - This means your model is learning and generalizing better on the training data.  \n",
    "2. **Macro F1 Score Increasing** – From **0.6084 → 0.6867 → 0.6986**  \n",
    "   - A steady increase in F1 indicates that the model is improving in terms of balanced performance across all classes.  \n",
    "3. **Validation Loss Fluctuating** – Slight variation (**0.652 → 0.601 → 0.603**)  \n",
    "   - While it decreased initially, the slight increase in epoch 3 suggests potential overfitting.  \n",
    "\n",
    "The model has completed **3 epochs** with the following results:\n",
    "\n",
    "### **Key Metrics**  \n",
    "- **Training Loss:** **0.6478** (Good, decreasing trend)  \n",
    "- **Training Time:** **~63,062 seconds (~17.5 hours)** (Seems quite long because i dont have GPU)  \n",
    "- **Samples per second:** **0.476** (Very slow, likely due to large model size and CPU usage)  \n",
    "- **Steps per second:** **0.03** (Very low, may need optimization)  \n",
    "- **Total FLOPs:** **~1.97 quadrillion** (High computational cost)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883687ea-20d8-49bb-9fa0-d3be70ec8a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "# Final evaluation\n",
    "results = trainer.evaluate(test_dataset)\n",
    "print(\"\\n🔹 Test Set Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14fda94d-810c-4809-b2e3-0307d4d88e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sentiment_model\\\\tokenizer_config.json',\n",
       " 'sentiment_model\\\\special_tokens_map.json',\n",
       " 'sentiment_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model and tokenizer\n",
    "trainer.save_model(\"sentiment_model\")\n",
    "tokenizer.save_pretrained(\"sentiment_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2988e992-1892-4ae9-9ce6-cd3d3c8a9543",
   "metadata": {},
   "source": [
    "import shutil\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Ensure the 'sentiment_model' folder exists\n",
    "model_dir = \"sentiment_model\"\n",
    "\n",
    "# Zip the 'sentiment_model' folder\n",
    "shutil.make_archive(model_dir, 'zip', model_dir)\n",
    "\n",
    "# Provide a download link to the zip file\n",
    "FileLink(f\"{model_dir}.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aee5812-f8b9-4cdc-b354-7715cd8605a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
